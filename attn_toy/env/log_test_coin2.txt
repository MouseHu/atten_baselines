/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/policies.py:203: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/tf_layers.py:136: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/acktr.py:181: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:418: calling extract_image_patches (from tensorflow.python.ops.array_ops) with ksizes is deprecated and will be removed in a future version.
Instructions for updating:
ksizes is deprecated, use sizes instead
WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/acktr.py:223: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

basic check finished
---------------------------------
| explained_variance | 0.105    |
| fps                | 9        |
| nupdates           | 1        |
| policy_entropy     | 1.11     |
| policy_loss        | -1.33    |
| total_timesteps    | 20       |
| value_loss         | 1.8      |
---------------------------------
---------------------------------
| ep_len_mean        | 85.6     |
| ep_reward_mean     | -4.17    |
| explained_variance | -47.7    |
| fps                | 40       |
| nupdates           | 100      |
| policy_entropy     | 1.14     |
| policy_loss        | -0.201   |
| total_timesteps    | 2000     |
| value_loss         | 0.0902   |
---------------------------------
---------------------------------
| ep_len_mean        | 87.6     |
| ep_reward_mean     | -4.28    |
| explained_variance | -7.02    |
| fps                | 39       |
| nupdates           | 200      |
| policy_entropy     | 1.33     |
| policy_loss        | -0.122   |
| total_timesteps    | 4000     |
| value_loss         | 0.00877  |
---------------------------------
----------------------------------
| ep_len_mean        | 86.4      |
| ep_reward_mean     | -3.08     |
| explained_variance | -2.97e+05 |
| fps                | 38        |
| nupdates           | 300       |
| policy_entropy     | 1.23      |
| policy_loss        | -0.122    |
| total_timesteps    | 6000      |
| value_loss         | 0.0122    |
----------------------------------
---------------------------------
| ep_len_mean        | 87.1     |
| ep_reward_mean     | -2.83    |
| explained_variance | -0.677   |
| fps                | 37       |
| nupdates           | 400      |
| policy_entropy     | 1.09     |
| policy_loss        | -0.101   |
| total_timesteps    | 8000     |
| value_loss         | 0.00961  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.8     |
| ep_reward_mean     | -2.62    |
| explained_variance | -0.311   |
| fps                | 37       |
| nupdates           | 500      |
| policy_entropy     | 1.36     |
| policy_loss        | -0.088   |
| total_timesteps    | 10000    |
| value_loss         | 0.00408  |
---------------------------------
---------------------------------
| ep_len_mean        | 88.9     |
| ep_reward_mean     | -2.22    |
| explained_variance | 0.0552   |
| fps                | 37       |
| nupdates           | 600      |
| policy_entropy     | 1.32     |
| policy_loss        | 12.3     |
| total_timesteps    | 12000    |
| value_loss         | 84.5     |
---------------------------------
---------------------------------
| ep_len_mean        | 90.5     |
| ep_reward_mean     | -1.88    |
| explained_variance | -5.3     |
| fps                | 37       |
| nupdates           | 700      |
| policy_entropy     | 1.31     |
| policy_loss        | -0.154   |
| total_timesteps    | 14000    |
| value_loss         | 0.0188   |
---------------------------------
---------------------------------
| ep_len_mean        | 90.7     |
| ep_reward_mean     | -1.39    |
| explained_variance | -0.144   |
| fps                | 38       |
| nupdates           | 800      |
| policy_entropy     | 1.29     |
| policy_loss        | 0.36     |
| total_timesteps    | 16000    |
| value_loss         | 4.94     |
---------------------------------
---------------------------------
| ep_len_mean        | 91.4     |
| ep_reward_mean     | -0.956   |
| explained_variance | -568     |
| fps                | 38       |
| nupdates           | 900      |
| policy_entropy     | 1.33     |
| policy_loss        | -0.125   |
| total_timesteps    | 18000    |
| value_loss         | 0.0562   |
---------------------------------
---------------------------------
| ep_len_mean        | 90.1     |
| ep_reward_mean     | -0.424   |
| explained_variance | -0.72    |
| fps                | 38       |
| nupdates           | 1000     |
| policy_entropy     | 1.29     |
| policy_loss        | -0.00613 |
| total_timesteps    | 20000    |
| value_loss         | 0.000974 |
---------------------------------
---------------------------------
| ep_len_mean        | 86.3     |
| ep_reward_mean     | 0.255    |
| explained_variance | -0.888   |
| fps                | 38       |
| nupdates           | 1100     |
| policy_entropy     | 1.21     |
| policy_loss        | -0.0368  |
| total_timesteps    | 22000    |
| value_loss         | 0.000569 |
---------------------------------
---------------------------------
| ep_len_mean        | 87.3     |
| ep_reward_mean     | 0.156    |
| explained_variance | -3.02    |
| fps                | 38       |
| nupdates           | 1200     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.00385 |
| total_timesteps    | 24000    |
| value_loss         | 0.000136 |
---------------------------------
---------------------------------
| ep_len_mean        | 87.3     |
| ep_reward_mean     | -0.244   |
| explained_variance | -0.0604  |
| fps                | 39       |
| nupdates           | 1300     |
| policy_entropy     | 1.36     |
| policy_loss        | 10.1     |
| total_timesteps    | 26000    |
| value_loss         | 68.1     |
---------------------------------
---------------------------------
| ep_len_mean        | 86       |
| ep_reward_mean     | -0.118   |
| explained_variance | -13.2    |
| fps                | 39       |
| nupdates           | 1400     |
| policy_entropy     | 1.25     |
| policy_loss        | 0.117    |
| total_timesteps    | 28000    |
| value_loss         | 0.0109   |
---------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -0.665   |
| explained_variance | -8.61    |
| fps                | 39       |
| nupdates           | 1500     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0844  |
| total_timesteps    | 30000    |
| value_loss         | 0.00524  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.8     |
| ep_reward_mean     | -0.496   |
| explained_variance | -0.047   |
| fps                | 39       |
| nupdates           | 1600     |
| policy_entropy     | 1.37     |
| policy_loss        | 5.67     |
| total_timesteps    | 32000    |
| value_loss         | 39.3     |
---------------------------------
---------------------------------
| ep_len_mean        | 86.5     |
| ep_reward_mean     | -0.162   |
| explained_variance | -0.00433 |
| fps                | 40       |
| nupdates           | 1700     |
| policy_entropy     | 1.33     |
| policy_loss        | 1.94     |
| total_timesteps    | 34000    |
| value_loss         | 14.6     |
---------------------------------
---------------------------------
| ep_len_mean        | 87.5     |
| ep_reward_mean     | 0.039    |
| explained_variance | -9.97    |
| fps                | 40       |
| nupdates           | 1800     |
| policy_entropy     | 1.38     |
| policy_loss        | -0.0644  |
| total_timesteps    | 36000    |
| value_loss         | 0.00369  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.9     |
| ep_reward_mean     | -0.21    |
| explained_variance | -3.21    |
| fps                | 40       |
| nupdates           | 1900     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.00487 |
| total_timesteps    | 38000    |
| value_loss         | 0.000268 |
---------------------------------
---------------------------------
| ep_len_mean        | 85.9     |
| ep_reward_mean     | 0.197    |
| explained_variance | -0.497   |
| fps                | 40       |
| nupdates           | 2000     |
| policy_entropy     | 1.34     |
| policy_loss        | -0.0263  |
| total_timesteps    | 40000    |
| value_loss         | 0.000583 |
---------------------------------
---------------------------------
| ep_len_mean        | 84.9     |
| ep_reward_mean     | 0.6      |
| explained_variance | 0.00397  |
| fps                | 40       |
| nupdates           | 2100     |
| policy_entropy     | 1.27     |
| policy_loss        | 8.06     |
| total_timesteps    | 42000    |
| value_loss         | 79.8     |
---------------------------------
---------------------------------
| ep_len_mean        | 86.1     |
| ep_reward_mean     | -0.024   |
| explained_variance | -0.609   |
| fps                | 41       |
| nupdates           | 2200     |
| policy_entropy     | 1.28     |
| policy_loss        | -0.0582  |
| total_timesteps    | 44000    |
| value_loss         | 0.00224  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.6     |
| ep_reward_mean     | -0.28    |
| explained_variance | -1.43    |
| fps                | 41       |
| nupdates           | 2300     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0523  |
| total_timesteps    | 46000    |
| value_loss         | 0.00296  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.1     |
| ep_reward_mean     | -0.027   |
| explained_variance | -2.76    |
| fps                | 41       |
| nupdates           | 2400     |
| policy_entropy     | 1.34     |
| policy_loss        | -0.0481  |
| total_timesteps    | 48000    |
| value_loss         | 0.00187  |
---------------------------------
----------------------------------
| ep_len_mean        | 91.8      |
| ep_reward_mean     | -0.896    |
| explained_variance | -50.8     |
| fps                | 41        |
| nupdates           | 2500      |
| policy_entropy     | 1.37      |
| policy_loss        | -0.000522 |
| total_timesteps    | 50000     |
| value_loss         | 0.000282  |
----------------------------------
---------------------------------
| ep_len_mean        | 89.9     |
| ep_reward_mean     | -1.11    |
| explained_variance | -0.764   |
| fps                | 41       |
| nupdates           | 2600     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.115   |
| total_timesteps    | 52000    |
| value_loss         | 0.00743  |
---------------------------------
---------------------------------
| ep_len_mean        | 90.1     |
| ep_reward_mean     | -0.63    |
| explained_variance | -0.355   |
| fps                | 41       |
| nupdates           | 2700     |
| policy_entropy     | 1.28     |
| policy_loss        | -0.118   |
| total_timesteps    | 54000    |
| value_loss         | 0.00736  |
---------------------------------
---------------------------------
| ep_len_mean        | 89.8     |
| ep_reward_mean     | -0.998   |
| explained_variance | -206     |
| fps                | 42       |
| nupdates           | 2800     |
| policy_entropy     | 1.36     |
| policy_loss        | 0.0174   |
| total_timesteps    | 56000    |
| value_loss         | 0.000856 |
---------------------------------
---------------------------------
| ep_len_mean        | 87.4     |
| ep_reward_mean     | -1.16    |
| explained_variance | -0.296   |
| fps                | 42       |
| nupdates           | 2900     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0821  |
| total_timesteps    | 58000    |
| value_loss         | 0.00365  |
---------------------------------
---------------------------------
| ep_len_mean        | 84.8     |
| ep_reward_mean     | -1.01    |
| explained_variance | -0.603   |
| fps                | 42       |
| nupdates           | 3000     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0473  |
| total_timesteps    | 60000    |
| value_loss         | 0.00115  |
---------------------------------
---------------------------------
| ep_len_mean        | 83.8     |
| ep_reward_mean     | -0.808   |
| explained_variance | -0.225   |
| fps                | 42       |
| nupdates           | 3100     |
| policy_entropy     | 1.15     |
| policy_loss        | 2.13     |
| total_timesteps    | 62000    |
| value_loss         | 24.8     |
---------------------------------
---------------------------------
| ep_len_mean        | 83.7     |
| ep_reward_mean     | -0.893   |
| explained_variance | -4.95    |
| fps                | 42       |
| nupdates           | 3200     |
| policy_entropy     | 1.27     |
| policy_loss        | 0.0243   |
| total_timesteps    | 64000    |
| value_loss         | 0.00269  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -0.359   |
| explained_variance | -0.0191  |
| fps                | 42       |
| nupdates           | 3300     |
| policy_entropy     | 1.36     |
| policy_loss        | 3.33     |
| total_timesteps    | 66000    |
| value_loss         | 25       |
---------------------------------
---------------------------------
| ep_len_mean        | 84.5     |
| ep_reward_mean     | -0.071   |
| explained_variance | -1.25    |
| fps                | 42       |
| nupdates           | 3400     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0621  |
| total_timesteps    | 68000    |
| value_loss         | 0.00185  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.6     |
| ep_reward_mean     | 0.026    |
| explained_variance | -13.8    |
| fps                | 42       |
| nupdates           | 3500     |
| policy_entropy     | 1.32     |
| policy_loss        | -0.00401 |
| total_timesteps    | 70000    |
| value_loss         | 0.00043  |
---------------------------------
---------------------------------
| ep_len_mean        | 88.2     |
| ep_reward_mean     | -0.634   |
| explained_variance | 0.0483   |
| fps                | 42       |
| nupdates           | 3600     |
| policy_entropy     | 1.35     |
| policy_loss        | 7.97     |
| total_timesteps    | 72000    |
| value_loss         | 51.3     |
---------------------------------
---------------------------------
| ep_len_mean        | 87       |
| ep_reward_mean     | -0.516   |
| explained_variance | -4.42    |
| fps                | 42       |
| nupdates           | 3700     |
| policy_entropy     | 1.26     |
| policy_loss        | -0.0558  |
| total_timesteps    | 74000    |
| value_loss         | 0.00196  |
---------------------------------
---------------------------------
| ep_len_mean        | 89.1     |
| ep_reward_mean     | -0.524   |
| explained_variance | -0.00584 |
| fps                | 41       |
| nupdates           | 3800     |
| policy_entropy     | 1.24     |
| policy_loss        | 12.5     |
| total_timesteps    | 76000    |
| value_loss         | 166      |
---------------------------------
---------------------------------
| ep_len_mean        | 90       |
| ep_reward_mean     | -0.824   |
| explained_variance | -65.8    |
| fps                | 41       |
| nupdates           | 3900     |
| policy_entropy     | 1.2      |
| policy_loss        | -0.091   |
| total_timesteps    | 78000    |
| value_loss         | 0.00435  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.1     |
| ep_reward_mean     | -0.429   |
| explained_variance | -6.95    |
| fps                | 41       |
| nupdates           | 4000     |
| policy_entropy     | 1.35     |
| policy_loss        | 0.0411   |
| total_timesteps    | 80000    |
| value_loss         | 0.00189  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.1     |
| ep_reward_mean     | -0.634   |
| explained_variance | -53.6    |
| fps                | 41       |
| nupdates           | 4100     |
| policy_entropy     | 1.35     |
| policy_loss        | 0.0107   |
| total_timesteps    | 82000    |
| value_loss         | 0.00198  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -0.456   |
| explained_variance | -13.9    |
| fps                | 41       |
| nupdates           | 4200     |
| policy_entropy     | 1.21     |
| policy_loss        | -0.158   |
| total_timesteps    | 84000    |
| value_loss         | 0.0186   |
---------------------------------
---------------------------------
| ep_len_mean        | 86.7     |
| ep_reward_mean     | -0.388   |
| explained_variance | -3.09    |
| fps                | 40       |
| nupdates           | 4300     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0245  |
| total_timesteps    | 86000    |
| value_loss         | 0.001    |
---------------------------------
---------------------------------
| ep_len_mean        | 88.6     |
| ep_reward_mean     | -0.074   |
| explained_variance | -0.144   |
| fps                | 40       |
| nupdates           | 4400     |
| policy_entropy     | 1.21     |
| policy_loss        | 6.32     |
| total_timesteps    | 88000    |
| value_loss         | 59.4     |
---------------------------------
---------------------------------
| ep_len_mean        | 88.8     |
| ep_reward_mean     | 0.31     |
| explained_variance | -3.63    |
| fps                | 40       |
| nupdates           | 4500     |
| policy_entropy     | 1.26     |
| policy_loss        | -0.0853  |
| total_timesteps    | 90000    |
| value_loss         | 0.00519  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.9     |
| ep_reward_mean     | 0.902    |
| explained_variance | -109     |
| fps                | 39       |
| nupdates           | 4600     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.0594  |
| total_timesteps    | 92000    |
| value_loss         | 0.00161  |
---------------------------------
---------------------------------
| ep_len_mean        | 84.2     |
| ep_reward_mean     | 0.673    |
| explained_variance | -0.00441 |
| fps                | 39       |
| nupdates           | 4700     |
| policy_entropy     | 1.35     |
| policy_loss        | 1.61     |
| total_timesteps    | 94000    |
| value_loss         | 9.99     |
---------------------------------
---------------------------------
| ep_len_mean        | 83.3     |
| ep_reward_mean     | 1.06     |
| explained_variance | -1.06    |
| fps                | 38       |
| nupdates           | 4800     |
| policy_entropy     | 1.17     |
| policy_loss        | -0.0148  |
| total_timesteps    | 96000    |
| value_loss         | 0.000271 |
---------------------------------
---------------------------------
| ep_len_mean        | 84.9     |
| ep_reward_mean     | 0.603    |
| explained_variance | -0.334   |
| fps                | 37       |
| nupdates           | 4900     |
| policy_entropy     | 1.32     |
| policy_loss        | -0.164   |
| total_timesteps    | 98000    |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| ep_len_mean        | 82.4     |
| ep_reward_mean     | 1.15     |
| explained_variance | -1.44    |
| fps                | 36       |
| nupdates           | 5000     |
| policy_entropy     | 1.36     |
| policy_loss        | 0.0166   |
| total_timesteps    | 100000   |
| value_loss         | 0.00133  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.1     |
| ep_reward_mean     | 1.8      |
| explained_variance | -18.3    |
| fps                | 35       |
| nupdates           | 5100     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0541  |
| total_timesteps    | 102000   |
| value_loss         | 0.00439  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.1     |
| ep_reward_mean     | 2.1      |
| explained_variance | -9.75    |
| fps                | 34       |
| nupdates           | 5200     |
| policy_entropy     | 1.19     |
| policy_loss        | -0.0507  |
| total_timesteps    | 104000   |
| value_loss         | 0.00276  |
---------------------------------
---------------------------------
| ep_len_mean        | 83.8     |
| ep_reward_mean     | 2.02     |
| explained_variance | 0.0207   |
| fps                | 33       |
| nupdates           | 5300     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0606  |
| total_timesteps    | 106000   |
| value_loss         | 0.00147  |
---------------------------------
---------------------------------
| ep_len_mean        | 84       |
| ep_reward_mean     | 1.6      |
| explained_variance | 0.125    |
| fps                | 32       |
| nupdates           | 5400     |
| policy_entropy     | 1.3      |
| policy_loss        | -0.049   |
| total_timesteps    | 108000   |
| value_loss         | 0.000886 |
---------------------------------
---------------------------------
| ep_len_mean        | 85.8     |
| ep_reward_mean     | 0.61     |
| explained_variance | -55      |
| fps                | 32       |
| nupdates           | 5500     |
| policy_entropy     | 1.28     |
| policy_loss        | 0.0491   |
| total_timesteps    | 110000   |
| value_loss         | 0.0146   |
---------------------------------
---------------------------------
| ep_len_mean        | 84.8     |
| ep_reward_mean     | -0.304   |
| explained_variance | -145     |
| fps                | 31       |
| nupdates           | 5600     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.081   |
| total_timesteps    | 112000   |
| value_loss         | 0.00368  |
---------------------------------
---------------------------------
| ep_len_mean        | 82.7     |
| ep_reward_mean     | -0.295   |
| explained_variance | -2.38    |
| fps                | 30       |
| nupdates           | 5700     |
| policy_entropy     | 1.35     |
| policy_loss        | 0.0326   |
| total_timesteps    | 114000   |
| value_loss         | 0.00198  |
---------------------------------
---------------------------------
| ep_len_mean        | 83       |
| ep_reward_mean     | 0.391    |
| explained_variance | -1.28    |
| fps                | 29       |
| nupdates           | 5800     |
| policy_entropy     | 1.29     |
| policy_loss        | -0.0926  |
| total_timesteps    | 116000   |
| value_loss         | 0.00506  |
---------------------------------
---------------------------------
| ep_len_mean        | 84.5     |
| ep_reward_mean     | -0.171   |
| explained_variance | -11.7    |
| fps                | 29       |
| nupdates           | 5900     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0789  |
| total_timesteps    | 118000   |
| value_loss         | 0.0048   |
---------------------------------
---------------------------------
| ep_len_mean        | 82.4     |
| ep_reward_mean     | 0.543    |
| explained_variance | -0.616   |
| fps                | 28       |
| nupdates           | 6000     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0317  |
| total_timesteps    | 120000   |
| value_loss         | 0.000924 |
---------------------------------
---------------------------------
| ep_len_mean        | 84.5     |
| ep_reward_mean     | 0.337    |
| explained_variance | -0.0207  |
| fps                | 28       |
| nupdates           | 6100     |
| policy_entropy     | 1.32     |
| policy_loss        | 8.09     |
| total_timesteps    | 122000   |
| value_loss         | 69.5     |
---------------------------------
---------------------------------
| ep_len_mean        | 88.1     |
| ep_reward_mean     | -0.429   |
| explained_variance | -3.36    |
| fps                | 27       |
| nupdates           | 6200     |
| policy_entropy     | 1.26     |
| policy_loss        | -0.0672  |
| total_timesteps    | 124000   |
| value_loss         | 0.00342  |
---------------------------------
---------------------------------
| ep_len_mean        | 90       |
| ep_reward_mean     | -0.513   |
| explained_variance | -5.18    |
| fps                | 27       |
| nupdates           | 6300     |
| policy_entropy     | 1.32     |
| policy_loss        | -0.0969  |
| total_timesteps    | 126000   |
| value_loss         | 0.00594  |
---------------------------------
---------------------------------
| ep_len_mean        | 89.9     |
| ep_reward_mean     | -0.712   |
| explained_variance | 0.00712  |
| fps                | 26       |
| nupdates           | 6400     |
| policy_entropy     | 1.34     |
| policy_loss        | 5.4      |
| total_timesteps    | 128000   |
| value_loss         | 39.8     |
---------------------------------
---------------------------------
| ep_len_mean        | 90.3     |
| ep_reward_mean     | -1.16    |
| explained_variance | -0.00233 |
| fps                | 26       |
| nupdates           | 6500     |
| policy_entropy     | 1.26     |
| policy_loss        | 2.7      |
| total_timesteps    | 130000   |
| value_loss         | 18       |
---------------------------------
---------------------------------
| ep_len_mean        | 87.4     |
| ep_reward_mean     | -0.156   |
| explained_variance | -10.3    |
| fps                | 26       |
| nupdates           | 6600     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0209  |
| total_timesteps    | 132000   |
| value_loss         | 0.00179  |
---------------------------------
----------------------------------
| ep_len_mean        | 81.1      |
| ep_reward_mean     | -0.134    |
| explained_variance | -3.89e+03 |
| fps                | 25        |
| nupdates           | 6700      |
| policy_entropy     | 1.35      |
| policy_loss        | 0.0617    |
| total_timesteps    | 134000    |
| value_loss         | 0.0041    |
----------------------------------
---------------------------------
| ep_len_mean        | 80.5     |
| ep_reward_mean     | 0.531    |
| explained_variance | -5.07    |
| fps                | 25       |
| nupdates           | 6800     |
| policy_entropy     | 1.34     |
| policy_loss        | -0.114   |
| total_timesteps    | 136000   |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| ep_len_mean        | 80.7     |
| ep_reward_mean     | 1.02     |
| explained_variance | -0.97    |
| fps                | 25       |
| nupdates           | 6900     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.112   |
| total_timesteps    | 138000   |
| value_loss         | 0.00732  |
---------------------------------
Traceback (most recent call last):
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1356, in _do_call
    return fn(*args)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[{{node kfac_apply/cond/SelfAdjointEigV2_7}}]]
  (1) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[{{node kfac_apply/cond/SelfAdjointEigV2_7}}]]
	 [[kfac_apply/cond_1/MatMul_12/Switch/_405]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "fourrooms_coin_norender.py", line 289, in <module>
    """
  File "/home/lzy/atten_baselines/stable_baselines/acktr/acktr.py", line 356, in learn
    writer)
  File "/home/lzy/atten_baselines/stable_baselines/acktr/acktr.py", line 281, in _train_step
    [self.pg_loss, self.vf_loss, self.entropy, self.train_op], td_map)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 950, in run
    run_metadata_ptr)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1350, in _do_run
    run_metadata)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[node kfac_apply/cond/SelfAdjointEigV2_7 (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:621) ]]
  (1) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[node kfac_apply/cond/SelfAdjointEigV2_7 (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:621) ]]
	 [[kfac_apply/cond_1/MatMul_12/Switch/_405]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node kfac_apply/cond/SelfAdjointEigV2_7:
 kfac/cond/Merge (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:531)	
 kfac_apply/AssignAdd (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:896)

Input Source operations connected to node kfac_apply/cond/SelfAdjointEigV2_7:
 kfac/cond/Merge (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:531)	
 kfac_apply/AssignAdd (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:896)

Original stack trace for 'kfac_apply/cond/SelfAdjointEigV2_7':
  File "fourrooms_coin_norender.py", line 289, in <module>
    """
  File "/home/lzy/atten_baselines/stable_baselines/acktr/acktr.py", line 310, in learn
    self.train_op, self.q_runner = self.optim.apply_gradients(list(zip(self.grads_check, self.params)))
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 994, in apply_gradients
    kfac_optim_op, queue_runner = self.apply_gradients_kfac(grads)
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 919, in apply_gradients_kfac
    no_op_wrapper)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1977, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1814, in BuildCondBranch
    original_result = fn()
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 918, in <lambda>
    lambda: tf.group(*self.apply_stats_eigen(self.compute_stats_eigen())),
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 621, in compute_stats_eigen
    eigen_decomposition = tf.self_adjoint_eig(stats_var)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/linalg_ops.py", line 328, in self_adjoint_eig
    e, v = gen_linalg_ops.self_adjoint_eig_v2(tensor, compute_v=True, name=name)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/gen_linalg_ops.py", line 2168, in self_adjoint_eig_v2
    "SelfAdjointEigV2", input=input, compute_v=compute_v, name=name)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

