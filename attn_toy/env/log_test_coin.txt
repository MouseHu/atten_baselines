/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/policies.py:203: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/common/tf_layers.py:136: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/acktr.py:181: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:418: calling extract_image_patches (from tensorflow.python.ops.array_ops) with ksizes is deprecated and will be removed in a future version.
Instructions for updating:
ksizes is deprecated, use sizes instead
WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/acktr.py:223: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

Box(0, 255, (39, 39, 3), uint8)
Wrapping the env in a DummyVecEnv.
---------------------------------
| explained_variance | 0.132    |
| fps                | 10       |
| nupdates           | 1        |
| policy_entropy     | 0.691    |
| policy_loss        | -0.954   |
| total_timesteps    | 20       |
| value_loss         | 1.09     |
---------------------------------
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -5.45    |
| explained_variance | -0.0131  |
| fps                | 38       |
| nupdates           | 100      |
| policy_entropy     | 1.18     |
| policy_loss        | 10.6     |
| total_timesteps    | 2000     |
| value_loss         | 76.1     |
---------------------------------
---------------------------------
| ep_len_mean        | 94.5     |
| ep_reward_mean     | -4.64    |
| explained_variance | -4.82    |
| fps                | 37       |
| nupdates           | 200      |
| policy_entropy     | 1.31     |
| policy_loss        | 0.0437   |
| total_timesteps    | 4000     |
| value_loss         | 0.0136   |
---------------------------------
---------------------------------
| ep_len_mean        | 88.2     |
| ep_reward_mean     | -2.04    |
| explained_variance | -136     |
| fps                | 36       |
| nupdates           | 300      |
| policy_entropy     | 0.96     |
| policy_loss        | -0.0395  |
| total_timesteps    | 6000     |
| value_loss         | 0.00496  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -1.81    |
| explained_variance | -3.5     |
| fps                | 36       |
| nupdates           | 400      |
| policy_entropy     | 1.27     |
| policy_loss        | -0.0311  |
| total_timesteps    | 8000     |
| value_loss         | 0.000712 |
---------------------------------
---------------------------------
| ep_len_mean        | 84       |
| ep_reward_mean     | -1.53    |
| explained_variance | -0.00199 |
| fps                | 36       |
| nupdates           | 500      |
| policy_entropy     | 1.24     |
| policy_loss        | 17.7     |
| total_timesteps    | 10000    |
| value_loss         | 255      |
---------------------------------
---------------------------------
| ep_len_mean        | 83.8     |
| ep_reward_mean     | -1.1     |
| explained_variance | -6.76    |
| fps                | 36       |
| nupdates           | 600      |
| policy_entropy     | 1.32     |
| policy_loss        | -0.0717  |
| total_timesteps    | 12000    |
| value_loss         | 0.00239  |
---------------------------------
---------------------------------
| ep_len_mean        | 83       |
| ep_reward_mean     | -1.44    |
| explained_variance | -4.62    |
| fps                | 36       |
| nupdates           | 700      |
| policy_entropy     | 1.35     |
| policy_loss        | -0.11    |
| total_timesteps    | 14000    |
| value_loss         | 0.0138   |
---------------------------------
---------------------------------
| ep_len_mean        | 86.6     |
| ep_reward_mean     | -1.19    |
| explained_variance | -0.0202  |
| fps                | 36       |
| nupdates           | 800      |
| policy_entropy     | 1.35     |
| policy_loss        | 3.84     |
| total_timesteps    | 16000    |
| value_loss         | 57.9     |
---------------------------------
---------------------------------
| ep_len_mean        | 86.1     |
| ep_reward_mean     | -1.04    |
| explained_variance | -243     |
| fps                | 36       |
| nupdates           | 900      |
| policy_entropy     | 1.37     |
| policy_loss        | -0.00262 |
| total_timesteps    | 18000    |
| value_loss         | 0.0021   |
---------------------------------
---------------------------------
| ep_len_mean        | 88.5     |
| ep_reward_mean     | -1.18    |
| explained_variance | 8.76e-05 |
| fps                | 36       |
| nupdates           | 1000     |
| policy_entropy     | 1.36     |
| policy_loss        | 6.13     |
| total_timesteps    | 20000    |
| value_loss         | 41.7     |
---------------------------------
---------------------------------
| ep_len_mean        | 89.5     |
| ep_reward_mean     | -0.764   |
| explained_variance | -0.00066 |
| fps                | 36       |
| nupdates           | 1100     |
| policy_entropy     | 1.35     |
| policy_loss        | 3.37     |
| total_timesteps    | 22000    |
| value_loss         | 24.5     |
---------------------------------
---------------------------------
| ep_len_mean        | 88.5     |
| ep_reward_mean     | -1.07    |
| explained_variance | 0.0134   |
| fps                | 36       |
| nupdates           | 1200     |
| policy_entropy     | 1.31     |
| policy_loss        | -0.0178  |
| total_timesteps    | 24000    |
| value_loss         | 0.000204 |
---------------------------------
---------------------------------
| ep_len_mean        | 86.9     |
| ep_reward_mean     | -0.913   |
| explained_variance | -7.66    |
| fps                | 36       |
| nupdates           | 1300     |
| policy_entropy     | 1.27     |
| policy_loss        | -0.0675  |
| total_timesteps    | 26000    |
| value_loss         | 0.00297  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.3     |
| ep_reward_mean     | -0.554   |
| explained_variance | -166     |
| fps                | 36       |
| nupdates           | 1400     |
| policy_entropy     | 1.3      |
| policy_loss        | -0.0227  |
| total_timesteps    | 28000    |
| value_loss         | 0.000314 |
---------------------------------
---------------------------------
| ep_len_mean        | 88.5     |
| ep_reward_mean     | -0.573   |
| explained_variance | -478     |
| fps                | 37       |
| nupdates           | 1500     |
| policy_entropy     | 1.33     |
| policy_loss        | -0.0744  |
| total_timesteps    | 30000    |
| value_loss         | 0.00348  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.8     |
| ep_reward_mean     | -0.903   |
| explained_variance | -5.92    |
| fps                | 37       |
| nupdates           | 1600     |
| policy_entropy     | 1.38     |
| policy_loss        | -0.0692  |
| total_timesteps    | 32000    |
| value_loss         | 0.00307  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.6     |
| ep_reward_mean     | -0.683   |
| explained_variance | 0.0576   |
| fps                | 37       |
| nupdates           | 1700     |
| policy_entropy     | 1.29     |
| policy_loss        | -0.109   |
| total_timesteps    | 34000    |
| value_loss         | 0.00695  |
---------------------------------
---------------------------------
| ep_len_mean        | 88.2     |
| ep_reward_mean     | -1.35    |
| explained_variance | -3.91    |
| fps                | 37       |
| nupdates           | 1800     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.0701  |
| total_timesteps    | 36000    |
| value_loss         | 0.00341  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.5     |
| ep_reward_mean     | -1.58    |
| explained_variance | -2.1     |
| fps                | 38       |
| nupdates           | 1900     |
| policy_entropy     | 1.33     |
| policy_loss        | -0.133   |
| total_timesteps    | 38000    |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| ep_len_mean        | 85.3     |
| ep_reward_mean     | -1.06    |
| explained_variance | -0.0559  |
| fps                | 38       |
| nupdates           | 2000     |
| policy_entropy     | 1.31     |
| policy_loss        | 11       |
| total_timesteps    | 40000    |
| value_loss         | 74.3     |
---------------------------------
---------------------------------
| ep_len_mean        | 85.2     |
| ep_reward_mean     | -0.644   |
| explained_variance | -7.22    |
| fps                | 38       |
| nupdates           | 2100     |
| policy_entropy     | 1.14     |
| policy_loss        | -0.0223  |
| total_timesteps    | 42000    |
| value_loss         | 0.000516 |
---------------------------------
---------------------------------
| ep_len_mean        | 86       |
| ep_reward_mean     | -0.521   |
| explained_variance | -11.5    |
| fps                | 38       |
| nupdates           | 2200     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.118   |
| total_timesteps    | 44000    |
| value_loss         | 0.00866  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.6     |
| ep_reward_mean     | -0.378   |
| explained_variance | -2.63    |
| fps                | 38       |
| nupdates           | 2300     |
| policy_entropy     | 1.36     |
| policy_loss        | 0.0528   |
| total_timesteps    | 46000    |
| value_loss         | 0.00367  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -0.764   |
| explained_variance | -0.0298  |
| fps                | 39       |
| nupdates           | 2400     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.13    |
| total_timesteps    | 48000    |
| value_loss         | 0.00825  |
---------------------------------
---------------------------------
| ep_len_mean        | 85.6     |
| ep_reward_mean     | -0.376   |
| explained_variance | -30.9    |
| fps                | 39       |
| nupdates           | 2500     |
| policy_entropy     | 1.25     |
| policy_loss        | -0.0303  |
| total_timesteps    | 50000    |
| value_loss         | 0.000489 |
---------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -0.361   |
| explained_variance | -8.04    |
| fps                | 39       |
| nupdates           | 2600     |
| policy_entropy     | 1.07     |
| policy_loss        | -0.0173  |
| total_timesteps    | 52000    |
| value_loss         | 0.000476 |
---------------------------------
---------------------------------
| ep_len_mean        | 84.8     |
| ep_reward_mean     | -0.799   |
| explained_variance | -0.389   |
| fps                | 39       |
| nupdates           | 2700     |
| policy_entropy     | 1.33     |
| policy_loss        | -0.0306  |
| total_timesteps    | 54000    |
| value_loss         | 0.00227  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.9     |
| ep_reward_mean     | -1.51    |
| explained_variance | -2.46    |
| fps                | 39       |
| nupdates           | 2800     |
| policy_entropy     | 1.2      |
| policy_loss        | 0.00639  |
| total_timesteps    | 56000    |
| value_loss         | 0.00134  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.1     |
| ep_reward_mean     | -1.84    |
| explained_variance | -5       |
| fps                | 40       |
| nupdates           | 2900     |
| policy_entropy     | 1.38     |
| policy_loss        | -0.0255  |
| total_timesteps    | 58000    |
| value_loss         | 0.00266  |
---------------------------------
---------------------------------
| ep_len_mean        | 88.9     |
| ep_reward_mean     | -2.83    |
| explained_variance | -37.1    |
| fps                | 40       |
| nupdates           | 3000     |
| policy_entropy     | 1.33     |
| policy_loss        | -0.00951 |
| total_timesteps    | 60000    |
| value_loss         | 0.00219  |
---------------------------------
---------------------------------
| ep_len_mean        | 91.4     |
| ep_reward_mean     | -2.78    |
| explained_variance | -0.102   |
| fps                | 40       |
| nupdates           | 3100     |
| policy_entropy     | 1.35     |
| policy_loss        | 9.71     |
| total_timesteps    | 62000    |
| value_loss         | 67.6     |
---------------------------------
---------------------------------
| ep_len_mean        | 91.1     |
| ep_reward_mean     | -2.04    |
| explained_variance | -0.0513  |
| fps                | 40       |
| nupdates           | 3200     |
| policy_entropy     | 1.37     |
| policy_loss        | 3.33     |
| total_timesteps    | 64000    |
| value_loss         | 22.1     |
---------------------------------
---------------------------------
| ep_len_mean        | 90.8     |
| ep_reward_mean     | -2.01    |
| explained_variance | -0.827   |
| fps                | 40       |
| nupdates           | 3300     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0104  |
| total_timesteps    | 66000    |
| value_loss         | 0.000706 |
---------------------------------
---------------------------------
| ep_len_mean        | 89.7     |
| ep_reward_mean     | -1.59    |
| explained_variance | -1.08    |
| fps                | 40       |
| nupdates           | 3400     |
| policy_entropy     | 1.27     |
| policy_loss        | -0.0569  |
| total_timesteps    | 68000    |
| value_loss         | 0.00188  |
---------------------------------
---------------------------------
| ep_len_mean        | 88.2     |
| ep_reward_mean     | -1.04    |
| explained_variance | -709     |
| fps                | 40       |
| nupdates           | 3500     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0344  |
| total_timesteps    | 70000    |
| value_loss         | 0.00126  |
---------------------------------
---------------------------------
| ep_len_mean        | 88       |
| ep_reward_mean     | -1.83    |
| explained_variance | -0.0174  |
| fps                | 40       |
| nupdates           | 3600     |
| policy_entropy     | 1.22     |
| policy_loss        | 4.61     |
| total_timesteps    | 72000    |
| value_loss         | 28.3     |
---------------------------------
---------------------------------
| ep_len_mean        | 86.5     |
| ep_reward_mean     | -1.27    |
| explained_variance | -0.00298 |
| fps                | 41       |
| nupdates           | 3700     |
| policy_entropy     | 1.28     |
| policy_loss        | -0.0547  |
| total_timesteps    | 74000    |
| value_loss         | 0.0016   |
---------------------------------
----------------------------------
| ep_len_mean        | 83.7      |
| ep_reward_mean     | -0.289    |
| explained_variance | -4.73e+03 |
| fps                | 41        |
| nupdates           | 3800      |
| policy_entropy     | 1.33      |
| policy_loss        | -0.0216   |
| total_timesteps    | 76000     |
| value_loss         | 0.00114   |
----------------------------------
---------------------------------
| ep_len_mean        | 85.4     |
| ep_reward_mean     | -0.759   |
| explained_variance | -0.146   |
| fps                | 41       |
| nupdates           | 3900     |
| policy_entropy     | 1.33     |
| policy_loss        | 12.3     |
| total_timesteps    | 78000    |
| value_loss         | 81.4     |
---------------------------------
---------------------------------
| ep_len_mean        | 85.7     |
| ep_reward_mean     | -0.089   |
| explained_variance | -2.35    |
| fps                | 41       |
| nupdates           | 4000     |
| policy_entropy     | 1.32     |
| policy_loss        | -0.0838  |
| total_timesteps    | 80000    |
| value_loss         | 0.00479  |
---------------------------------
---------------------------------
| ep_len_mean        | 87.8     |
| ep_reward_mean     | -0.09    |
| explained_variance | -17.8    |
| fps                | 41       |
| nupdates           | 4100     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0609  |
| total_timesteps    | 82000    |
| value_loss         | 0.00203  |
---------------------------------
---------------------------------
| ep_len_mean        | 89.4     |
| ep_reward_mean     | -0.151   |
| explained_variance | -0.00826 |
| fps                | 41       |
| nupdates           | 4200     |
| policy_entropy     | 1.26     |
| policy_loss        | 3.82     |
| total_timesteps    | 84000    |
| value_loss         | 58.1     |
---------------------------------
---------------------------------
| ep_len_mean        | 88.2     |
| ep_reward_mean     | 0.574    |
| explained_variance | -35.6    |
| fps                | 41       |
| nupdates           | 4300     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.07    |
| total_timesteps    | 86000    |
| value_loss         | 0.00643  |
---------------------------------
---------------------------------
| ep_len_mean        | 86.5     |
| ep_reward_mean     | 0.644    |
| explained_variance | -184     |
| fps                | 40       |
| nupdates           | 4400     |
| policy_entropy     | 1.33     |
| policy_loss        | -0.0484  |
| total_timesteps    | 88000    |
| value_loss         | 0.00103  |
---------------------------------
---------------------------------
| ep_len_mean        | 83.4     |
| ep_reward_mean     | 0.45     |
| explained_variance | -0.024   |
| fps                | 40       |
| nupdates           | 4500     |
| policy_entropy     | 1.38     |
| policy_loss        | 11.7     |
| total_timesteps    | 90000    |
| value_loss         | 76.2     |
---------------------------------
---------------------------------
| ep_len_mean        | 81.5     |
| ep_reward_mean     | 0.135    |
| explained_variance | -62.3    |
| fps                | 40       |
| nupdates           | 4600     |
| policy_entropy     | 1.38     |
| policy_loss        | -0.00795 |
| total_timesteps    | 92000    |
| value_loss         | 0.000266 |
---------------------------------
---------------------------------
| ep_len_mean        | 80.8     |
| ep_reward_mean     | -0.304   |
| explained_variance | 0.0455   |
| fps                | 40       |
| nupdates           | 4700     |
| policy_entropy     | 1.36     |
| policy_loss        | 12.1     |
| total_timesteps    | 94000    |
| value_loss         | 78.6     |
---------------------------------
---------------------------------
| ep_len_mean        | 83       |
| ep_reward_mean     | -0.323   |
| explained_variance | -2.07    |
| fps                | 40       |
| nupdates           | 4800     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.0564  |
| total_timesteps    | 96000    |
| value_loss         | 0.00229  |
---------------------------------
---------------------------------
| ep_len_mean        | 82.6     |
| ep_reward_mean     | 0.225    |
| explained_variance | -110     |
| fps                | 39       |
| nupdates           | 4900     |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0453  |
| total_timesteps    | 98000    |
| value_loss         | 0.0025   |
---------------------------------
---------------------------------
| ep_len_mean        | 84       |
| ep_reward_mean     | 0.386    |
| explained_variance | -19.2    |
| fps                | 38       |
| nupdates           | 5000     |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0737  |
| total_timesteps    | 100000   |
| value_loss         | 0.00244  |
---------------------------------
Traceback (most recent call last):
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1356, in _do_call
    return fn(*args)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[{{node kfac_apply/cond/SelfAdjointEigV2_9}}]]
  (1) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[{{node kfac_apply/cond/SelfAdjointEigV2_9}}]]
	 [[kfac_apply/cond_1/MatMul_12/Switch/_405]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "fourrooms_coin_norender.py", line 293, in <module>
    model.learn(total_timesteps=300000)
  File "/home/lzy/atten_baselines/stable_baselines/acktr/acktr.py", line 356, in learn
    writer)
  File "/home/lzy/atten_baselines/stable_baselines/acktr/acktr.py", line 281, in _train_step
    [self.pg_loss, self.vf_loss, self.entropy, self.train_op], td_map)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 950, in run
    run_metadata_ptr)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1350, in _do_run
    run_metadata)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[node kfac_apply/cond/SelfAdjointEigV2_9 (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:621) ]]
  (1) Invalid argument: Self-adjoint eigen decomposition was not successful. The input might not be valid.
	 [[node kfac_apply/cond/SelfAdjointEigV2_9 (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:621) ]]
	 [[kfac_apply/cond_1/MatMul_12/Switch/_405]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node kfac_apply/cond/SelfAdjointEigV2_9:
 kfac_apply/AssignAdd (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:896)	
 kfac/cond/Merge (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:531)

Input Source operations connected to node kfac_apply/cond/SelfAdjointEigV2_9:
 kfac_apply/AssignAdd (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:896)	
 kfac/cond/Merge (defined at /home/lzy/atten_baselines/stable_baselines/acktr/kfac.py:531)

Original stack trace for 'kfac_apply/cond/SelfAdjointEigV2_9':
  File "fourrooms_coin_norender.py", line 293, in <module>
    model.learn(total_timesteps=300000)
  File "/home/lzy/atten_baselines/stable_baselines/acktr/acktr.py", line 310, in learn
    self.train_op, self.q_runner = self.optim.apply_gradients(list(zip(self.grads_check, self.params)))
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 994, in apply_gradients
    kfac_optim_op, queue_runner = self.apply_gradients_kfac(grads)
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 919, in apply_gradients_kfac
    no_op_wrapper)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1977, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1814, in BuildCondBranch
    original_result = fn()
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 918, in <lambda>
    lambda: tf.group(*self.apply_stats_eigen(self.compute_stats_eigen())),
  File "/home/lzy/atten_baselines/stable_baselines/acktr/kfac.py", line 621, in compute_stats_eigen
    eigen_decomposition = tf.self_adjoint_eig(stats_var)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/linalg_ops.py", line 328, in self_adjoint_eig
    e, v = gen_linalg_ops.self_adjoint_eig_v2(tensor, compute_v=True, name=name)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/ops/gen_linalg_ops.py", line 2168, in self_adjoint_eig_v2
    "SelfAdjointEigV2", input=input, compute_v=compute_v, name=name)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/home/lzy/miniconda3/envs/sbaselines/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

